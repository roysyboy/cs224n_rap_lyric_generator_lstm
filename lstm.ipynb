{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch\n",
        "!curl -s https://course.fast.ai/setup/colab"
      ],
      "metadata": {
        "id": "t1tey0LvI_8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSVDkCsPS4ub",
        "outputId": "0c324315-e90a-4b9a-a487-f50ed4252965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm import tqdm\n",
        "# from model import Model\n",
        "# from dataset import Dataset\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.current_device())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "m7J3jIcpS4ue"
      },
      "outputs": [],
      "source": [
        "### Class: Dataset \n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        device,\n",
        "        args,\n",
        "    ):\n",
        "        self.device = device\n",
        "        \n",
        "        self.args = args\n",
        "        self.words = self.load_words()\n",
        "        self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "    def load_words(self):\n",
        "        text = []\n",
        "        with open('eminem_lyrics.txt', encoding=\"utf-8\") as f:\n",
        "        # with open('eminem_punc_cleaned.txt', encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                text += line.split(\" \")\n",
        "        return text # we return a list of string\n",
        "\n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.args[2]\n",
        "        # return len(self.words_indexes) - self.args.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            # torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),\n",
        "            # torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),\n",
        "            torch.tensor(self.words_indexes[index:index+self.args[2]]).to(self.device),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.args[2]+1]).to(self.device),\n",
        "            # torch.tensor(self.words_indexes[index:index+self.args[2]], device=device),\n",
        "            # torch.tensor(self.words_indexes[index+1:index+self.args[2]+1], device=device),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "K7oO_IwhS4uf"
      },
      "outputs": [],
      "source": [
        "### Class: Model\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(Model, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 1\n",
        "\n",
        "        self.device = dataset.device\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_size,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers\n",
        "            # dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "IZc4czezS4uf"
      },
      "outputs": [],
      "source": [
        "### function: train\n",
        "\n",
        "def train(dataset, model, args):\n",
        "    model.train()\n",
        "\n",
        "    # dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=args[1])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.93)\n",
        "\n",
        "    # for epoch in tqdm(range(args.max_epochs), total=args.max_epochs):\n",
        "    for epoch in tqdm(range(args[0]), total=args[0], position=0, leave=True):\n",
        "        state_h, state_c = model.init_state(args[2])\n",
        "\n",
        "        pbar = tqdm(enumerate(dataloader), total=len(dataloader), position=0, leave=True)\n",
        "        pbar.set_description(\"Epoch {}\".format(epoch))\n",
        "        for cnt, (x, y) in pbar:\n",
        "            x = x.to(model.device)\n",
        "            y = y.to(model.device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "        \n",
        "        scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "9Beg5X2ZS4ug"
      },
      "outputs": [],
      "source": [
        "### function: predict\n",
        "\n",
        "def predict(dataset, model, text, num_lines=16, min_words=5, max_words=15):\n",
        "    # model.eval()\n",
        "\n",
        "    words = text.split(' ')\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "    cur_word_cnt = len(words)\n",
        "    cur_line_cnt = i = 0\n",
        "\n",
        "    while cur_line_cnt < num_lines:\n",
        "        while cur_word_cnt <= max_words:\n",
        "            x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(model.device)\n",
        "            # x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]], device=device)\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "            last_word_logits = y_pred[0][-1]\n",
        "            p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "            word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "            if dataset.index_to_word[word_index][-1] == '\\n':\n",
        "                if cur_word_cnt < min_words:\n",
        "                    while dataset.index_to_word[word_index][-1] == '\\n':\n",
        "                        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "                else:\n",
        "                    cur_word_cnt = 0\n",
        "                    cur_line_cnt += 1\n",
        "                words.append(dataset.index_to_word[word_index])\n",
        "            elif cur_word_cnt >= max_words:\n",
        "                words.append('\\n')\n",
        "                cur_word_cnt = 0\n",
        "                cur_line_cnt += 1\n",
        "            else:\n",
        "                words.append(dataset.index_to_word[word_index])\n",
        "            cur_word_cnt += 1\n",
        "            if cur_line_cnt > num_lines:\n",
        "                break\n",
        "            i += 1\n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "oKmDJWcNS4uh",
        "outputId": "ccf97ada-dec5-476d-c522-54f7364ad20b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:   0%|          | 0/661 [00:00<?, ?it/s]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-dbd93a38fa38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m## Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-162-874e31a9495d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, args)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 786.00 MiB (GPU 0; 11.17 GiB total capacity; 10.31 GiB already allocated; 173.81 MiB free; 10.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "### WHAT WE ACTUALLY NEED TO RUN: ###\n",
        "\n",
        "## Set up args\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--max-epochs', type=int, default=10)\n",
        "# parser.add_argument('--batch-size', type=int, default=128)\n",
        "# parser.add_argument('--sequence-length', type=int, default=8)\n",
        "# args = parser.parse_args()\n",
        "# (max-epochs, batch-size, sequence-length)\n",
        "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "## Set up dataset\n",
        "max_epochs = 10\n",
        "batch_size = 512\n",
        "sequence_length = 16\n",
        "args = (max_epochs, batch_size, sequence_length)\n",
        "dataset = Dataset(device, args)\n",
        "# dataset.to(device)\n",
        "\n",
        "## Set up model\n",
        "model = Model(dataset)\n",
        "model = model.to(device)\n",
        "\n",
        "## Train model\n",
        "train(dataset, model, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBsnYSNTS4ui",
        "outputId": "d888dfb1-1736-4348-be4f-2be74599e778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [Eminem]:\n",
            " his palms are sweaty patience trl promised raps like here and i ain't stopping from \n",
            " the beat i'm search of school kill kids\n",
            " my you drained me i'm hittin' home of this slump i'm pissed off i \n",
            " you think i got so zone at 'em 'til the fuckin' cause by the \n",
            " my fault anyway in the nightmare i never late for the other side\n",
            " caught in a chase 25 to life\n",
            " too late for the other side\n",
            " caught in a chase 25 to life\n",
            " too much is the fat who who who never pushed 'em home with dre \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate text\n",
        "num_lines = 8\n",
        "text = \"his palms are sweaty\"\n",
        "prediction = \" \".join(predict(dataset, model, text=text, num_lines=num_lines))\n",
        "print(\" [Eminem]:\")\n",
        "print(\"\", prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "AXzzJ1rtS4ui"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}